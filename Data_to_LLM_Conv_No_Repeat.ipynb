{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import r2_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional\n",
    "## delete celltypes with cell counts lower than certain amount\n",
    "## specifing an lower bond is a must\n",
    "def filter_low_counts(celltype_df, age_df, celltype_col, threshold):\n",
    "    print(\"Checking low count cell types...\")\n",
    "    \n",
    "    celltype_count = Counter(celltype_df[celltype_col])\n",
    "    for key in celltype_count:\n",
    "        if threshold == None:\n",
    "            unique_ages = np.unique(age_df)\n",
    "            num_groups = (len(unique_ages) + 1) * 100\n",
    "            if celltype_count[key] < num_groups:\n",
    "                print(key, \" has too low counts\")\n",
    "                celltype_df = celltype_df[celltype_df[celltype_col] != key]\n",
    "        else:\n",
    "            if celltype_count[key] < threshold:\n",
    "                print(key, \" has too low counts\")\n",
    "                celltype_df = celltype_df[celltype_df[celltype_col] != key]\n",
    "    return celltype_df\n",
    "\n",
    "def get_skewed_count_info(adata, class_col, age_col, age_threshold):\n",
    "    print(\"Checking skewed count cell types...\")\n",
    "    \n",
    "    # Compute the fraction of cells for each age group within each cell ontology class\n",
    "    group_counts = adata.obs.groupby([class_col, age_col]).size()\n",
    "    total_counts = adata.obs.groupby([class_col]).size()\n",
    "    \n",
    "    # Calculate the fraction of each age group within each class\n",
    "    class_age_fraction = group_counts / total_counts\n",
    "    \n",
    "    # Find the cell classes to filter out based on age distribution\n",
    "    classes_to_filter = class_age_fraction[class_age_fraction > age_threshold].index.get_level_values(0).unique()\n",
    "    \n",
    "    return classes_to_filter\n",
    "\n",
    "## Read h5ad file \n",
    "## and do cell type filtering based on age distribution and cell count thresholds.\n",
    "def read_and_filter_h5ad(filepath_1, filepath_2 = None, class_col=\"celltype\", age_col=\"age\", age_threshold=0.8, count_threshold=None):\n",
    "    \"\"\"Parameters:\n",
    "    adata: AnnData object\n",
    "        The Scanpy AnnData object containing single-cell data.\n",
    "    class_col: str, optional (default: 'celltype')\n",
    "        The column name in adata.obs representing the cell ontology class.\n",
    "    age_col: str, optional (default: 'age')\n",
    "        The column name in adata.obs representing the age of the cells.\n",
    "    age_threshold: float, optional (default: 0.8)\n",
    "        The threshold fraction for filtering based on age distribution. If one age group has more than this\n",
    "        fraction of cells in a class, the class will be filtered out.\n",
    "    count_threshold: list, optional (default: [100])\n",
    "        Threshold for filtering cell types based on count. If a single value is provided,\n",
    "        it filters out cell types with counts lower than this value. If a range is provided,\n",
    "        it filters out cell types outside this range.\n",
    "    \n",
    "    Returns:\n",
    "    filtered_adata: AnnData object\n",
    "        The filtered AnnData object with specified cell ontology classes removed based on both criteria.\"\"\"\n",
    "    try:\n",
    "        adata1 = sc.read_h5ad(filepath_1)\n",
    "        if filepath_2 != None:\n",
    "            adata2 = sc.read_h5ad(filepath_2)\n",
    "            adata1 = adata1.concatenate(adata2)\n",
    "        adata = adata1\n",
    "        \n",
    "        celltype_df = adata.obs[[class_col]].copy()\n",
    "        age_df = adata.obs[[age_col]].copy()\n",
    "        \n",
    "        # Apply the cell count threshold filtering\n",
    "        celltype_df = filter_low_counts(celltype_df, age_df, class_col, count_threshold)\n",
    "    \n",
    "        # Create a filtered AnnData object based on cell count filtering\n",
    "        filtered_adata = adata[celltype_df.index].copy()\n",
    "        \n",
    "        # Identify the skewed classes to filter based on age distribution\n",
    "        classes_to_filter = get_skewed_count_info(filtered_adata, class_col, age_col, age_threshold)\n",
    "        \n",
    "        if len(classes_to_filter):\n",
    "            print(classes_to_filter[0], \" has skewed cell counts\")\n",
    "        # Further filter the AnnData object based on age distribution\n",
    "        final_filtered_adata = filtered_adata[~filtered_adata.obs[class_col].isin(classes_to_filter)].copy()\n",
    "        \n",
    "        return final_filtered_adata\n",
    "    except Exception as e:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 path: /home/hang/SC_Ageing_Prediction/tabula-muris-senis-facs-processed-official-annotations-Brain_Myeloid.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hang/.local/lib/python3.10/site-packages/anndata/compat/__init__.py:371: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/home/hang/.local/lib/python3.10/site-packages/anndata/compat/__init__.py:371: FutureWarning: Moving element from .uns['neighbors']['connectivities'] to .obsp['connectivities'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/home/hang/.local/lib/python3.10/site-packages/anndata/compat/__init__.py:371: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/home/hang/.local/lib/python3.10/site-packages/anndata/compat/__init__.py:371: FutureWarning: Moving element from .uns['neighbors']['connectivities'] to .obsp['connectivities'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/tmp/ipykernel_17948/1320463345.py:61: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  adata1 = adata1.concatenate(adata2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking low count cell types...\n",
      "macrophage  has too low counts\n",
      "CD8-positive, alpha-beta T cell  has too low counts\n",
      "ependymal cell  has too low counts\n",
      "interneuron  has too low counts\n",
      "oligodendrocyte precursor cell  has too low counts\n",
      "Bergmann glial cell  has too low counts\n",
      "neuroepithelial cell  has too low counts\n",
      "T cell  has too low counts\n",
      "neuronal stem cell  has too low counts\n",
      "mature NK T cell  has too low counts\n",
      "medium spiny neuron  has too low counts\n",
      "Checking skewed count cell types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17948/1320463345.py:25: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  group_counts = adata.obs.groupby([class_col, age_col]).size()\n",
      "/tmp/ipykernel_17948/1320463345.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  total_counts = adata.obs.groupby([class_col]).size()\n"
     ]
    }
   ],
   "source": [
    "# current_dir = Path.cwd()\n",
    "# print(f\"Current working directory: {current_dir}\")\n",
    "current_dir = Path.cwd()\n",
    "file1 = current_dir / \"tabula-muris-senis-facs-processed-official-annotations-Brain_Myeloid.h5ad\"\n",
    "file2 = current_dir / \"tabula-muris-senis-facs-processed-official-annotations-Brain_Non-Myeloid.h5ad\"\n",
    "print(f\"File 1 path: {file1}\")\n",
    "# assert file1.is_file(), f\"File not found: {file1}\"\n",
    "adata = read_and_filter_h5ad(str(file1), str(file2), \"cell_ontology_class\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 19154 × 22966\n",
       "    obs: 'FACS.selection', 'age', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'free_annotation', 'method', 'mouse.id', 'sex', 'subtissue', 'tissue', 'n_genes', 'n_counts', 'louvain', 'leiden', 'batch'\n",
       "    var: 'n_cells', 'means-0', 'dispersions-0', 'dispersions_norm-0', 'highly_variable-0', 'means-1', 'dispersions-1', 'dispersions_norm-1', 'highly_variable-1'\n",
       "    obsm: 'X_pca', 'X_tsne', 'X_umap'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "A10_B001060_B009250_S214.mm10-plus-1-0-0    1505\n",
      "A10_B001061_B009251_S298.mm10-plus-1-0-0    2384\n",
      "A10_B002503_B009456_S10.mm10-plus-1-0-0      599\n",
      "A10_B002702_B009296_S154.mm10-plus-1-0-0    1547\n",
      "A10_D045853_B009304_S106.mm10-plus-1-0-0     931\n",
      "Name: n_genes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print out first 5 rows of the data\n",
    "print(adata.obs[\"n_genes\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = adata.X.toarray() if not isinstance(adata.X, np.ndarray) else adata.X\n",
    "\n",
    "# # Round values to nearest int and ensure non-negative\n",
    "# X = np.round(X).astype(int)\n",
    "# X[X < 0] = 0  # just in case\n",
    "\n",
    "# # Get gene names\n",
    "# genes = np.array(adata.var_names)\n",
    "\n",
    "# # Prepare the list of gene strings\n",
    "# gene_strings = []\n",
    "# for row in X:\n",
    "#     repeated_genes = np.repeat(genes, row)\n",
    "#     gene_string = \" \".join(repeated_genes)\n",
    "#     gene_strings.append(gene_string)\n",
    "\n",
    "# # Get relevant metadata\n",
    "# df_meta = adata.obs[['age', 'sex', 'cell_ontology_class', 'tissue']].copy()\n",
    "# df_meta.columns = ['age', 'gender', 'cell_ontology_class', 'tissue']\n",
    "\n",
    "# # Create final DataFrame\n",
    "# df_final = pd.DataFrame({\n",
    "#     'genes': gene_strings,\n",
    "#     'age': df_meta['age'].values,\n",
    "#     'gender': df_meta['gender'].values,\n",
    "#     'cell_ontology_class': df_meta['cell_ontology_class'].values,\n",
    "#     'tissue': df_meta['tissue'].values\n",
    "# })\n",
    "\n",
    "# # Set index as 0 to n-1\n",
    "# df_final.index = range(df_final.shape[0])\n",
    "\n",
    "# # Optional: Save to CSV\n",
    "# df_final.to_csv(\"processed_cells.csv\", index_label=\"index\")\n",
    "\n",
    "# # Return final DataFrame\n",
    "# df_final\n",
    "\n",
    "\n",
    "\n",
    "## too memory intensive \n",
    "## write to file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare output file\n",
    "# output_file = \"processed_cells_streamed.csv\"\n",
    "\n",
    "# # Get gene names\n",
    "# genes = np.array(adata.var_names)\n",
    "\n",
    "# # Get expression matrix (dense row-by-row)\n",
    "# X = adata.X\n",
    "\n",
    "# # Prepare metadata\n",
    "# ages = adata.obs[\"age\"].values\n",
    "# genders = adata.obs[\"sex\"].values\n",
    "# classes = adata.obs[\"cell_ontology_class\"].values\n",
    "# tissues = adata.obs[\"tissue\"].values\n",
    "\n",
    "# # Open file and write line-by-line\n",
    "# with open(output_file, mode='w', newline='', encoding='utf-8') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # Write header\n",
    "#     writer.writerow([\"index\", \"genes\", \"age\", \"gender\", \"cell_ontology_class\", \"tissue\"])\n",
    "\n",
    "#     for i in range(adata.n_obs):\n",
    "#         # Get row i as dense array\n",
    "#         row = X[i].toarray().flatten() if not isinstance(X, np.ndarray) else X[i]\n",
    "        \n",
    "#         # Convert to int and clip negatives\n",
    "#         row = np.maximum(np.round(row).astype(int), 0)\n",
    "\n",
    "#         # Efficiently repeat gene names\n",
    "#         repeated_genes = np.repeat(genes, row)\n",
    "#         gene_string = \" \".join(repeated_genes)\n",
    "\n",
    "#         # Write row to file\n",
    "#         writer.writerow([\n",
    "#             i,\n",
    "#             gene_string,\n",
    "#             ages[i],\n",
    "#             genders[i],\n",
    "#             classes[i],\n",
    "#             tissues[i]\n",
    "#         ])\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'3m': 7394, '18m': 6928, '24m': 4832})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json, math, os, pathlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ─── Pull data from AnnData ─────────────────────────────────────────────────────\n",
    "genes   = np.asarray(adata.var_names)\n",
    "X       = adata.X\n",
    "ages    = adata.obs[\"age\"].values\n",
    "genders = adata.obs[\"sex\"].values\n",
    "classes = adata.obs[\"cell_ontology_class\"].values\n",
    "tissues = adata.obs[\"tissue\"].values\n",
    "\n",
    "print(Counter(ages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ─── Split config ───────────────────────────────────────────────────────────────\n",
    "# N_SPLITS   = 11\n",
    "# SPLIT_SIZE = math.ceil(adata.n_obs / N_SPLITS)\n",
    "# OUT_DIR    = \"fine_tune_chunks\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# INSTRUCTION = \"Predict the age of a single cell from gene expression and metadata.\"\n",
    "\n",
    "# def bag_of_words(counts_row: np.ndarray, gene_names: np.ndarray) -> str:\n",
    "#     \"\"\"Convert a vector of counts to space-separated tokens.\"\"\"\n",
    "#     counts_row = np.maximum(np.round(counts_row).astype(int), 0)\n",
    "#     return \" \".join(np.repeat(gene_names, counts_row))\n",
    "\n",
    "# # ─── Build chunk files in **Alpaca** format ─────────────────────────────────────\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "# for split_idx, (_, split_indices) in enumerate(skf.split(np.zeros(len(ages)), ages)):\n",
    "#     records = []\n",
    "#     for i in split_indices:\n",
    "#         row = X[i].toarray().ravel() if not isinstance(X, np.ndarray) else X[i]\n",
    "#         cell_input = (\n",
    "#             f\"Genes: {bag_of_words(row, genes)}\\n\"\n",
    "#             f\"Gender: {genders[i]}\\n\"\n",
    "#             f\"Class: {classes[i]}\\n\"\n",
    "#             f\"Tissue: {tissues[i]}\"\n",
    "#         )\n",
    "\n",
    "#         records.append({\n",
    "#             \"instruction\": INSTRUCTION,\n",
    "#             \"input\": cell_input,\n",
    "#             \"output\": str(ages[i])\n",
    "#         })\n",
    "#         break\n",
    "# #     fname = os.path.join(OUT_DIR, f\"cell_data_part_{split_idx+1}.json\")\n",
    "# #     with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "# #         json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# #     print(f\"Wrote {len(records):>5} samples → {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##################################the following code will transform the gene symbols to a more readable format###################\n",
    "\n",
    "\n",
    "###################but I just realized that I am stupid and I can just add all gene symbols to the tokenizer and it will work fine######################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import inflect\n",
    "\n",
    "# p = inflect.engine()\n",
    "# global unseen_cnt\n",
    "# global seen_cnt\n",
    "# unseen_cnt = 0\n",
    "# seen_cnt = 0\n",
    "# # ─── Load gene symbol ➜ biotype_ID mapping ──────────────────────────────────\n",
    "# GENEINFO_PATH = \"geneInfo.tab\"  # adjust if the file lives elsewhere\n",
    "\n",
    "# gene_mapping: dict[str, str] = {}\n",
    "# with open(GENEINFO_PATH, \"r\", encoding=\"utf-8\") as fh:\n",
    "#     for raw_line in fh:\n",
    "#         line = raw_line.strip()\n",
    "#         # The first line of geneInfo.tab is just the row‑count (e.g. \"33696\") – skip it\n",
    "#         if not line or line.isdigit():\n",
    "#             continue\n",
    "#         try:\n",
    "#             gene_id, symbol, biotype = line.split(\"\\t\")\n",
    "#         except ValueError:  # line didn’t have three columns – ignore it\n",
    "#             continue\n",
    "#         # Keep the last 6 digits of the Ensembl ID and prepend the gene biotype\n",
    "#         #   ENSMUSG00000051951  →  \"051951\"\n",
    "#         numeric_id = gene_id[-6:]\n",
    "#         #gene_mapping[symbol] = f\"{biotype} {numeric_id}\"\n",
    "#         gene_mapping[symbol] = f\"{numeric_id}\"  # use only the numeric ID for simplicity\n",
    "\n",
    "\n",
    "# def translate_gene(symbol: str) -> str:\n",
    "#     \"\"\"Return the canonical training token for a gene symbol.\n",
    "\n",
    "#     Example\n",
    "#     -------\n",
    "#     >>> translate_gene(\"Xkr4\")\n",
    "#     'protein_coding_051951'\n",
    "#     \"\"\"\n",
    "#     new_name = gene_mapping.get(symbol, symbol)  # fall back to the symbol if unseen\n",
    "#     if new_name == symbol:\n",
    "#         global unseen_cnt\n",
    "#         # expand the unseen gene name into gene mapping\n",
    "#         # new_gene = f\"gene {str(unseen_cnt)}\"\n",
    "#         new_gene = f\"{str(unseen_cnt)}\"\n",
    "#         new_name = new_gene\n",
    "#         gene_mapping[symbol] = new_gene\n",
    "#         unseen_cnt += 1\n",
    "\n",
    "#     return new_name\n",
    "\n",
    "\n",
    "# # ─── Prompt details ──────────────────────────────────────────────────────────\n",
    "# INSTRUCTION = \"Predict the age of a single cell from gene expression and metadata.\"\n",
    "\n",
    "\n",
    "# def bag_of_words(counts_row: np.ndarray, gene_names: np.ndarray) -> str:\n",
    "#     \"\"\"Convert a vector of counts to \"gene english(count)\" tokens separated by space.\"\"\"\n",
    "#     counts_row = np.maximum(np.round(counts_row).astype(int), 0)\n",
    "#     tokens = [\n",
    "#         f\"{translate_gene(gene)} {p.number_to_words(count)}\"\n",
    "#         for gene, count in zip(gene_names, counts_row)\n",
    "#         if count > 0\n",
    "#     ]\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# # ─── Build chunk files in **Alpaca** format ───────────────────────────────────\n",
    "# N_SPLITS = 11\n",
    "# SPLIT_SIZE = math.ceil(adata.n_obs / N_SPLITS)\n",
    "# OUT_DIR = \"fine_tune_chunks\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# lengths = []\n",
    "# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "# for split_idx, (_, split_indices) in enumerate(skf.split(np.zeros(len(ages)), ages)):\n",
    "#     records = []\n",
    "#     for i in split_indices:\n",
    "#         row = X[i].toarray().ravel() if not isinstance(X, np.ndarray) else X[i]\n",
    "#         bow = bag_of_words(row, genes)\n",
    "#         lengths.append(len(bow))\n",
    "#         cell_input = (\n",
    "#             f\"Genes: {bow}\\n\"\n",
    "#             f\"Gender: {genders[i]}\\n\"\n",
    "#             f\"Class: {classes[i]}\\n\"\n",
    "#             f\"Tissue: {tissues[i]}\"\n",
    "#         )\n",
    "        \n",
    "#         records.append({\n",
    "#             \"instruction\": INSTRUCTION,\n",
    "#             \"input\": cell_input,\n",
    "#             \"output\": str(ages[i])\n",
    "#         })\n",
    "#         # print(\"unseen:\", unseen_cnt, \"seen:\", seen_cnt)\n",
    "\n",
    "#     fname = os.path.join(OUT_DIR, f\"cell_data_part_noname_{split_idx + 1}.json\")\n",
    "#     with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "#     print(f\"Wrote {len(records):>5} samples → {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote  1742 samples → fine_tune_chunks/cell_data_part_1.json\n",
      "Wrote  1742 samples → fine_tune_chunks/cell_data_part_2.json\n",
      "Wrote  1742 samples → fine_tune_chunks/cell_data_part_3.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_4.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_5.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_6.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_7.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_8.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_9.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_10.json\n",
      "Wrote  1741 samples → fine_tune_chunks/cell_data_part_11.json\n",
      "Discovered 22727 unique genes.\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "N_SPLITS   = 11\n",
    "SPLIT_SIZE = math.ceil(adata.n_obs / N_SPLITS)\n",
    "OUT_DIR    = \"fine_tune_chunks\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "INSTRUCTION = \"Predict the age of a single cell from gene expression and metadata.\"\n",
    "\n",
    "# ─── NEW ────\n",
    "gene_vocab: dict[str, int] = {}          # {gene_name: token_id}\n",
    "\n",
    "def bag_of_words(counts_row: np.ndarray,\n",
    "                 gene_names: np.ndarray,\n",
    "                 vocab: dict[str, int] = gene_vocab) -> str:\n",
    "    \"\"\"\n",
    "    Convert a vector of counts to 'gene english(count)' tokens separated by space\n",
    "    and populate `vocab` with any previously unseen gene names.\n",
    "    \"\"\"\n",
    "    counts_row = np.maximum(np.round(counts_row).astype(int), 0)\n",
    "\n",
    "    tokens = []\n",
    "    for gene, count in zip(gene_names, counts_row):\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        # keep track of first appearance\n",
    "        if gene not in vocab:\n",
    "            vocab[gene] = len(vocab)      # next free index\n",
    "\n",
    "        tokens.append(f\"{gene} {p.number_to_words(count)}\")\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "lengths = []\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "for split_idx, (_, split_indices) in enumerate(skf.split(np.zeros(len(ages)), ages)):\n",
    "    records = []\n",
    "\n",
    "    for i in split_indices:\n",
    "        row = X[i].toarray().ravel() if not isinstance(X, np.ndarray) else X[i]\n",
    "\n",
    "        bow = bag_of_words(row, genes)        # <- now also updates vocab\n",
    "        lengths.append(len(bow))\n",
    "\n",
    "        cell_input = (\n",
    "            f\"Genes: {bow}\\n\"\n",
    "            f\"Gender: {genders[i]}\\n\"\n",
    "            f\"Class: {classes[i]}\\n\"\n",
    "            f\"Tissue: {tissues[i]}\"\n",
    "        )\n",
    "        records.append({\n",
    "            \"instruction\": INSTRUCTION,\n",
    "            \"input\": cell_input,\n",
    "            \"output\": str(ages[i])\n",
    "        })\n",
    "\n",
    "    fname = os.path.join(OUT_DIR, f\"cell_data_part_{split_idx+1}.json\")\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Wrote {len(records):>5} samples → {fname}\")\n",
    "    # break\n",
    "\n",
    "# ─── Optionally save the new-token list for later use ─────────\n",
    "with open(os.path.join(OUT_DIR, \"gene_tokens.json\"), \"w\") as f:\n",
    "    json.dump(list(gene_vocab.keys()), f, indent=2)\n",
    "\n",
    "print(f\"Discovered {len(gene_vocab)} unique genes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70201, 681)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths), min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hang/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:933: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer repo – this happens once…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 51.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uilding enhanced tokenizer (one‑off)…\n",
      "   → Original vocab size: 151669\n",
      "   → Added 22727 new tokens (vocab now 174396)\n",
      "Enhanced tokenizer saved to /home/hang/.hf_cache/qwen3_14b/tokenizer_plus\n",
      "Vocab size: 174396\n"
     ]
    }
   ],
   "source": [
    "import shutil, os; shutil.rmtree(os.path.expanduser(\"~/.hf_cache/qwen3_14b\"), ignore_errors=True)\n",
    "############### use this to clear the cache if needed ###############\n",
    "\n",
    "import json, glob, itertools\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login, snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "REPO_ID  = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\n",
    "CACHE_DIR = Path.home() / \".hf_cache\" / \"qwen3_14b\"  # persists across sessions\n",
    "TOKENIZER_DIR = CACHE_DIR / \"tokenizer_plus\"           # where the enhanced files live\n",
    "\n",
    "# Respect the user's HF token, if provided via env‑var. Safer than hard‑coding.\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # → None if not set\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "\n",
    "# Speed up start‑up: no anonymous telemetry pings.\n",
    "os.environ.setdefault(\"HF_HUB_DISABLE_TELEMETRY\", \"1\")\n",
    "\n",
    "# Ensure the cache path exists\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "if not (CACHE_DIR / \"tokenizer.json\").exists():\n",
    "    print(\"Downloading tokenizer repo – this happens once…\")\n",
    "    snapshot_download(\n",
    "        repo_id=REPO_ID,\n",
    "        local_dir=str(CACHE_DIR),\n",
    "        local_dir_use_symlinks=False,   # real files – avoids broken symlinks inside containers\n",
    "        token=HF_TOKEN,                 # may be None (public repo)\n",
    "        ignore_patterns=[\"*.safetensors\", \"*.bin\"],  # skip model weights for now\n",
    "    )\n",
    "else:\n",
    "    print(\"Repo already cached – skipping download.\")\n",
    "\n",
    "\n",
    "def build_extra_tokens() -> list[str]:\n",
    "    # \"\"\"Return the list of tokens we want to add exactly once.\"\"\"\n",
    "    # digits_padded = [f\"{i:03d}\" for i in range(1000)]  # 0000 … 999\n",
    "    # digits_plain  = [str(i) for i in range(4_000)]        # 0 … 1999\n",
    "    # keywords      = [\" protein_coding \", \" lncRNA \", \" gene \"]\n",
    "    # Use dict.fromkeys to deduplicate *while* preserving order\n",
    "    OUT_DIR   = Path(\"fine_tune_chunks\")        # or whatever you used before\n",
    "    GENE_FILE = OUT_DIR / \"gene_tokens.json\"    # this is now a Path, not str\n",
    "\n",
    "    if GENE_FILE.exists():\n",
    "        with open(GENE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_genes: list[str] = json.load(f)\n",
    "        gene_tokens = [f\" {g} \" for g in raw_genes]       # <space>GENE<space>\n",
    "    else:\n",
    "        print(f\"⚠️  Gene token file {GENE_FILE} not found – proceeding without it.\")\n",
    "        gene_tokens = []\n",
    "\n",
    "    return list(dict.fromkeys(gene_tokens))\n",
    "   # return list(dict.fromkeys(digits_padded + digits_plain + keywords))\n",
    "\n",
    "if TOKENIZER_DIR.exists():\n",
    "    print(\"Loading enhanced tokenizer…\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, trust_remote_code=True)\n",
    "else:\n",
    "    print(\"uilding enhanced tokenizer (one‑off)…\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CACHE_DIR, trust_remote_code=True)\n",
    "    print(f\"   → Original vocab size: {len(tokenizer)}\")\n",
    "    extra_tokens = build_extra_tokens()\n",
    "    added = tokenizer.add_tokens(extra_tokens, special_tokens=False)\n",
    "    print(f\"   → Added {added} new tokens (vocab now {len(tokenizer)})\")\n",
    "    tokenizer.save_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "\n",
    "print(f\"Enhanced tokenizer saved to {TOKENIZER_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer))\n",
    "# print(\"Tokenize sample:\", tokenizer.tokenize(\" protein_coding 000123\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file #1: fine_tune_chunks/cell_data_part_10.json\n",
      "Current max token length: 0\n",
      "Maximum prompt length (tokens): 9949\n",
      "average tokens per item: 3063.382538770821\n",
      "Processing file #2: fine_tune_chunks/cell_data_part_8.json\n",
      "Current max token length: 9949\n",
      "Maximum prompt length (tokens): 12031\n",
      "average tokens per item: 3080.094485927628\n",
      "Processing file #3: fine_tune_chunks/cell_data_part_9.json\n",
      "Current max token length: 12031\n",
      "Maximum prompt length (tokens): 12529\n",
      "average tokens per item: 3081.0696917480377\n",
      "Processing file #4: fine_tune_chunks/cell_data_part_3.json\n",
      "Current max token length: 12529\n",
      "Maximum prompt length (tokens): 12529\n",
      "average tokens per item: 3080.2391959798997\n",
      "Processing file #5: fine_tune_chunks/cell_data_part_4.json\n",
      "Current max token length: 12529\n",
      "Maximum prompt length (tokens): 12529\n",
      "average tokens per item: 3084.909947162876\n",
      "Processing file #6: fine_tune_chunks/cell_data_part_5.json\n",
      "Current max token length: 12529\n",
      "Maximum prompt length (tokens): 12529\n",
      "average tokens per item: 3086.0756197951564\n",
      "Processing file #7: fine_tune_chunks/cell_data_part_6.json\n",
      "Current max token length: 12529\n",
      "Maximum prompt length (tokens): 12529\n",
      "average tokens per item: 3086.7571381686903\n",
      "Processing file #8: fine_tune_chunks/cell_data_part_1.json\n",
      "Current max token length: 12529\n",
      "Maximum prompt length (tokens): 12529\n",
      "average tokens per item: 3083.140344580043\n",
      "Processing file #9: fine_tune_chunks/cell_data_part_7.json\n",
      "Current max token length: 12529\n"
     ]
    }
   ],
   "source": [
    "def examples_in(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        first = next(itertools.dropwhile(str.isspace, f.read(1)), '')\n",
    "        f.seek(0)\n",
    "        if first == '[':          # normal JSON array\n",
    "            yield from json.load(f)\n",
    "        else:                     # JSON-Lines\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    yield json.loads(line)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. Scan all files and track the maximum prompt length in *tokens*\n",
    "max_tokens = 0\n",
    "cnt = 0\n",
    "total_items = 0\n",
    "average_tokens = []\n",
    "for path in glob.glob(\"fine_tune_chunks/cell_data_part_*.json\"):\n",
    "    cnt += 1\n",
    "    print(f\"Processing file #{cnt}: {path}\")\n",
    "    print(\"Current max token length:\", max_tokens)\n",
    "\n",
    "    for ex in examples_in(path):\n",
    "        total_items += 1\n",
    "        prompt = f\"{ex.get('instruction','')} {ex.get('input','')}\".strip()\n",
    "        token_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "        n_tokens = len(token_ids)\n",
    "\n",
    "       ##### # Print the first 10 *tokens* (not just their IDs)\n",
    "        # first_10_token_ids = token_ids[:200]\n",
    "        # first_10_tokens = tokenizer.convert_ids_to_tokens(first_10_token_ids)\n",
    "        # print(\"Prompt:\", prompt)\n",
    "        # print(\"First 10 tokens:\", first_10_tokens)\n",
    "        # # print 10 a line\n",
    "        # for i in range(0, len(first_10_tokens), 10):\n",
    "        #     print(\" \".join(first_10_tokens[i:i+10]))\n",
    "\n",
    "       ############ # Track the longest prompt length observed\n",
    "        max_tokens = max(max_tokens, n_tokens)\n",
    "        average_tokens.append(n_tokens)\n",
    "        # break\n",
    "    # break\n",
    "    print(f\"Maximum prompt length (tokens): {max_tokens}\")\n",
    "    print(f\"average tokens per item: {np.mean(average_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 6000 tokens: 48\n",
      "more than 7000 tokens: 13\n",
      "more than 8000 tokens: 6\n",
      "more than 9000 tokens: 3\n",
      "more than 10000 tokens: 0\n",
      "total items: 1742\n"
     ]
    }
   ],
   "source": [
    "print(f\"more than 6000 tokens: {len([x for x in average_tokens if x > 6000])}\")\n",
    "print(f\"more than 7000 tokens: {len([x for x in average_tokens if x > 7000])}\")\n",
    "print(f\"more than 8000 tokens: {len([x for x in average_tokens if x > 8000])}\")\n",
    "print(f\"more than 9000 tokens: {len([x for x in average_tokens if x > 9000])}\")\n",
    "print(f\"more than 10000 tokens: {len([x for x in average_tokens if x > 10000])}\")\n",
    "print(f\"total items: {len(average_tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
