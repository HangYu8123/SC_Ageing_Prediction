{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hang/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional\n",
    "## delete celltypes with cell counts lower than certain amount\n",
    "## specifing an lower bond is a must\n",
    "def filter_low_counts(celltype_df, age_df, celltype_col, threshold):\n",
    "    print(\"Checking low count cell types...\")\n",
    "    \n",
    "    celltype_count = Counter(celltype_df[celltype_col])\n",
    "    for key in celltype_count:\n",
    "        if threshold == None:\n",
    "            unique_ages = np.unique(age_df)\n",
    "            num_groups = (len(unique_ages) + 1) * 100\n",
    "            if celltype_count[key] < num_groups:\n",
    "                print(key, \" has too low counts\")\n",
    "                celltype_df = celltype_df[celltype_df[celltype_col] != key]\n",
    "        else:\n",
    "            if celltype_count[key] < threshold:\n",
    "                print(key, \" has too low counts\")\n",
    "                celltype_df = celltype_df[celltype_df[celltype_col] != key]\n",
    "    return celltype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skewed_count_info(adata, class_col, age_col, age_threshold):\n",
    "    print(\"Checking skewed count cell types...\")\n",
    "    \n",
    "    # Compute the fraction of cells for each age group within each cell ontology class\n",
    "    group_counts = adata.obs.groupby([class_col, age_col]).size()\n",
    "    total_counts = adata.obs.groupby([class_col]).size()\n",
    "    \n",
    "    # Calculate the fraction of each age group within each class\n",
    "    class_age_fraction = group_counts / total_counts\n",
    "    \n",
    "    # Find the cell classes to filter out based on age distribution\n",
    "    classes_to_filter = class_age_fraction[class_age_fraction > age_threshold].index.get_level_values(0).unique()\n",
    "    \n",
    "    return classes_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read h5ad file \n",
    "## and do cell type filtering based on age distribution and cell count thresholds.\n",
    "def read_and_filter_h5ad(filepath_1, filepath_2 = None, class_col=\"celltype\", age_col=\"age\", age_threshold=0.8, count_threshold=None):\n",
    "    \"\"\"Parameters:\n",
    "    adata: AnnData object\n",
    "        The Scanpy AnnData object containing single-cell data.\n",
    "    class_col: str, optional (default: 'celltype')\n",
    "        The column name in adata.obs representing the cell ontology class.\n",
    "    age_col: str, optional (default: 'age')\n",
    "        The column name in adata.obs representing the age of the cells.\n",
    "    age_threshold: float, optional (default: 0.8)\n",
    "        The threshold fraction for filtering based on age distribution. If one age group has more than this\n",
    "        fraction of cells in a class, the class will be filtered out.\n",
    "    count_threshold: list, optional (default: [100])\n",
    "        Threshold for filtering cell types based on count. If a single value is provided,\n",
    "        it filters out cell types with counts lower than this value. If a range is provided,\n",
    "        it filters out cell types outside this range.\n",
    "    \n",
    "    Returns:\n",
    "    filtered_adata: AnnData object\n",
    "        The filtered AnnData object with specified cell ontology classes removed based on both criteria.\"\"\"\n",
    "    try:\n",
    "        adata1 = sc.read_h5ad(filepath_1)\n",
    "        if adata2!=None:\n",
    "            adata2 = sc.read_h5ad(filepath_2)\n",
    "            adata1 = adata1.concatenate(adata2)\n",
    "        adata = adata1\n",
    "        \n",
    "        celltype_df = adata.obs[[class_col]].copy()\n",
    "        age_df = adata.obs[[age_col]].copy()\n",
    "        \n",
    "        # Apply the cell count threshold filtering\n",
    "        celltype_df = filter_low_counts(celltype_df, age_df, class_col, count_threshold)\n",
    "    \n",
    "        # Create a filtered AnnData object based on cell count filtering\n",
    "        filtered_adata = adata[celltype_df.index].copy()\n",
    "        \n",
    "        # Identify the skewed classes to filter based on age distribution\n",
    "        classes_to_filter = get_skewed_count_info(filtered_adata, class_col, age_col, age_threshold)\n",
    "        \n",
    "        if len(classes_to_filter):\n",
    "            print(classes_to_filter[0], \" has skewed cell counts\")\n",
    "        # Further filter the AnnData object based on age distribution\n",
    "        final_filtered_adata = filtered_adata[~filtered_adata.obs[class_col].isin(classes_to_filter)].copy()\n",
    "        \n",
    "        return final_filtered_adata\n",
    "    except Exception as e:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/hang/SC_Ageing_Prediction\n",
      "File 1 path: /home/hang/SC_Ageing_Prediction/tabula-muris-senis-facs-processed-official-annotations-Brain_Myeloid.h5ad\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory, this should get the path automatically\n",
    "# hope it works for mac\n",
    "current_dir = Path.cwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# Construct full file paths using the `/` operator\n",
    "file1 = current_dir / \"tabula-muris-senis-facs-processed-official-annotations-Brain_Myeloid.h5ad\"\n",
    "# file2 = current_dir / \"tabula-muris-senis-facs-processed-official-annotations-Brain_Non-Myeloid.h5ad\"\n",
    "# Print the paths for verification\n",
    "print(f\"File 1 path: {file1}\")\n",
    "# print(f\"File 2 path: {file2}\")\n",
    "\n",
    "# Verify that files exist\n",
    "assert file1.is_file(), f\"File not found: {file1}\"\n",
    "# assert file2.is_file(), f\"File not found: {file2}\"\n",
    "\n",
    "# Read and filter the data\n",
    "# adata = read_and_filter_h5ad(str(file1), str(file2), \"cell_ontology_class\", \"age\")\n",
    "\n",
    "# adata = read_and_filter_h5ad(\"../Mouse Tabula Muris/tabula-muris-senis-facs-processed-official-annotations-Brain_Myeloid.h5ad\", \n",
    "#                              \"../Mouse Tabula Muris/tabula-muris-senis-facs-processed-official-annotations-Brain_Non-Myeloid.h5ad\",\n",
    "#                              \"cell_ontology_class\", \"age\")\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'anndata._core.anndata.AnnData'>\n",
      "AnnData object with n_obs × n_vars = 19154 × 22966\n",
      "    obs: 'FACS.selection', 'age', 'cell', 'cell_ontology_class', 'cell_ontology_id', 'free_annotation', 'method', 'mouse.id', 'sex', 'subtissue', 'tissue', 'n_genes', 'n_counts', 'louvain', 'leiden', 'batch'\n",
      "    var: 'n_cells', 'means-0', 'dispersions-0', 'dispersions_norm-0', 'highly_variable-0', 'means-1', 'dispersions-1', 'dispersions_norm-1', 'highly_variable-1'\n",
      "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['FACS.selection', 'age', 'cell', 'cell_ontology_class',\n",
      "       'cell_ontology_id', 'free_annotation', 'method', 'mouse.id', 'sex',\n",
      "       'subtissue', 'tissue', 'n_genes', 'n_counts', 'louvain', 'leiden',\n",
      "       'batch'],\n",
      "      dtype='object')\n",
      "(19154, 2)\n",
      "  (1, 0)\t2.314707\n",
      "  (24, 0)\t4.270282\n",
      "  (33, 0)\t2.0799525\n",
      "  (87, 0)\t3.9693682\n",
      "  (91, 0)\t2.137829\n",
      "  (111, 0)\t4.3484535\n",
      "  (119, 0)\t1.5670886\n",
      "  (141, 0)\t1.7044617\n",
      "  (381, 0)\t5.772542\n",
      "  (544, 0)\t0.6663411\n",
      "  (592, 0)\t3.5498033\n",
      "  (621, 0)\t1.130082\n",
      "  (683, 0)\t1.6384314\n",
      "  (684, 0)\t0.20371401\n",
      "  (687, 0)\t1.908242\n",
      "  (746, 0)\t3.4922144\n",
      "  (758, 0)\t2.7498093\n",
      "  (765, 0)\t2.041962\n",
      "  (776, 0)\t3.9030762\n",
      "  (807, 0)\t2.4771554\n",
      "  (860, 0)\t4.2531247\n",
      "  (867, 0)\t2.8258338\n",
      "  (933, 0)\t0.8700883\n",
      "  (955, 0)\t3.9151838\n",
      "  (974, 0)\t0.6200936\n",
      "  :\t:\n",
      "  (22237, 19153)\t2.5425427\n",
      "  (22239, 19153)\t3.0523267\n",
      "  (22297, 19153)\t6.3414\n",
      "  (22307, 19153)\t3.3811102\n",
      "  (22310, 19153)\t2.515704\n",
      "  (22314, 19153)\t4.0386224\n",
      "  (22326, 19153)\t0.2426163\n",
      "  (22333, 19153)\t0.2590379\n",
      "  (22334, 19153)\t5.2271886\n",
      "  (22336, 19153)\t1.6320196\n",
      "  (22342, 19153)\t2.9365914\n",
      "  (22345, 19153)\t1.1978363\n",
      "  (22366, 19153)\t1.0325153\n",
      "  (22380, 19153)\t2.9680405\n",
      "  (22463, 19153)\t2.465488\n",
      "  (22503, 19153)\t4.414221\n",
      "  (22529, 19153)\t6.616345\n",
      "  (22727, 19153)\t2.6450999\n",
      "  (22756, 19153)\t5.6228237\n",
      "  (22804, 19153)\t9.052309\n",
      "  (22811, 19153)\t10.0\n",
      "  (22858, 19153)\t3.858448\n",
      "  (22886, 19153)\t7.53608\n",
      "  (22895, 19153)\t0.23421858\n",
      "  (22896, 19153)\t5.267689\n"
     ]
    }
   ],
   "source": [
    "# What does adata look like?\n",
    "\n",
    "print(type(adata))\n",
    "print(adata)\n",
    "print(type(adata.obs))\n",
    "# print out all obs columns\n",
    "print(adata.obs.columns)    \n",
    "print(adata.obsm['X_tsne'].shape)\n",
    "print(adata.X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACS.selection\n",
      "Microglia    8642\n",
      "nan          7394\n",
      "Neurons      3118\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "age\n",
      "3m     7394\n",
      "18m    6928\n",
      "24m    4832\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "cell\n",
      "A10_B001060                1\n",
      "O8.MAA000617.3_10_M.1.1    1\n",
      "O8.MAA000593.3_8_M.1.1     1\n",
      "O8.MAA000592.3_9_M.1.1     1\n",
      "O8.MAA000591.3_8_M.1.1     1\n",
      "                          ..\n",
      "B7_B003910                 1\n",
      "B7_B003907                 1\n",
      "B7_B003899                 1\n",
      "B6_B003914                 1\n",
      "P9.MAA001894.3_39_F.1.1    1\n",
      "Name: count, Length: 19154, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "cell_ontology_class\n",
      "microglial cell     13268\n",
      "endothelial cell     2232\n",
      "oligodendrocyte      2094\n",
      "astrocyte             592\n",
      "brain pericyte        484\n",
      "neuron                484\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "cell_ontology_id\n",
      "nan           11925\n",
      "CL:0000129     4394\n",
      "CL:0000128     1394\n",
      "CL:0000115      708\n",
      "CL:0000127      413\n",
      "CL:2000043      156\n",
      "CL:0000540      137\n",
      "CL:0000235       17\n",
      "CL:0000644       10\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "free_annotation\n",
      "nan                          16364\n",
      "Endothelial cell              2190\n",
      "Pericyte                       484\n",
      "Granular_Neuron_CB_Gabra6       74\n",
      "new-cluster                     42\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "method\n",
      "facs    19154\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "mouse.id\n",
      "18_46_F    2147\n",
      "18_47_F    1933\n",
      "3_10_M     1849\n",
      "18_45_M    1715\n",
      "24_60_M    1605\n",
      "3_9_M      1445\n",
      "3_38_F     1402\n",
      "24_58_M    1268\n",
      "18_53_M    1133\n",
      "24_59_M    1082\n",
      "3_8_M      1061\n",
      "3_39_F     1046\n",
      "24_61_M     877\n",
      "3_56_F      372\n",
      "3_11_M      219\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "sex\n",
      "male      12254\n",
      "female     6900\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "subtissue\n",
      "Cortex          3426\n",
      "Hippocampus     3312\n",
      "Striatum        3081\n",
      "Cerebellum      2407\n",
      "Cortex          2227\n",
      "Cerebellum      1744\n",
      "Striatum        1596\n",
      "Hippocampus     1361\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "tissue\n",
      "Brain_Myeloid        13130\n",
      "Brain_Non-Myeloid     6024\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "n_genes\n",
      "1748    19\n",
      "2308    19\n",
      "1805    19\n",
      "1769    19\n",
      "1891    19\n",
      "        ..\n",
      "4523     1\n",
      "4793     1\n",
      "4337     1\n",
      "3580     1\n",
      "5030     1\n",
      "Name: count, Length: 3771, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "n_counts\n",
      "123217.0     2\n",
      "616244.0     2\n",
      "19143.0      2\n",
      "1199152.0    2\n",
      "129216.0     2\n",
      "            ..\n",
      "1241428.0    1\n",
      "3997412.0    1\n",
      "763549.0     1\n",
      "1054703.0    1\n",
      "122397.0     1\n",
      "Name: count, Length: 19090, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "louvain\n",
      "0     2101\n",
      "1     1913\n",
      "2     1683\n",
      "3     1532\n",
      "4     1446\n",
      "5     1379\n",
      "7     1236\n",
      "10    1180\n",
      "8     1171\n",
      "6     1036\n",
      "11    1033\n",
      "9      980\n",
      "12     824\n",
      "13     702\n",
      "14     595\n",
      "16     251\n",
      "17      81\n",
      "15       9\n",
      "18       2\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "leiden\n",
      "0     1634\n",
      "1     1526\n",
      "2     1425\n",
      "3     1379\n",
      "4     1357\n",
      "5     1306\n",
      "6     1202\n",
      "7     1077\n",
      "9      820\n",
      "13     781\n",
      "8      756\n",
      "14     743\n",
      "12     732\n",
      "11     715\n",
      "15     706\n",
      "10     608\n",
      "16     594\n",
      "17     594\n",
      "18     419\n",
      "19     385\n",
      "21     243\n",
      "20      85\n",
      "22      32\n",
      "23      28\n",
      "24       7\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n",
      "batch\n",
      "0    13130\n",
      "1     6024\n",
      "Name: count, dtype: int64\n",
      "***************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out all obs columns\n",
    "for col in adata.obs.columns:\n",
    "    print(adata.obs[col].value_counts())\n",
    "    print(\"***************************************************\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# print(adata.obs['age'].value_counts())\n",
    "\n",
    "# question: why there is not 18m here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hang Yu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\scanpy\\preprocessing\\_simple.py:523: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[key_n_counts] = counts_per_cell\n",
      "C:\\Users\\Hang Yu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "sc.pp.filter_genes(adata, min_cells=5)\n",
    "sc.pp.filter_cells(adata, min_genes=500)\n",
    "adata.obs['n_counts'] = np.sum(adata.X, axis=1).A1\n",
    "adata = adata[adata.obs['n_counts']>=3000]\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4) #simple lib size normalization?\n",
    "adata = sc.pp.filter_genes_dispersion(adata, subset = False, min_disp=.5, max_disp=None, \n",
    "                              min_mean=.0125, max_mean=10, n_bins=20, n_top_genes=None, \n",
    "                              log=True, copy=True)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.scale(adata, max_value=10, zero_center=False)\n",
    "sc.tl.pca(adata,use_highly_variable=True)\n",
    "sc.pp.neighbors(adata, n_neighbors=18)\n",
    "sc.tl.louvain(adata, resolution = 1)\n",
    "sc.tl.umap(adata)\n",
    "if 'X_umap' not in adata.obsm.keys():\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_pca')\n",
    "    sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [3]\n",
      " [3]\n",
      " ...\n",
      " [2]\n",
      " [5]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Index(['FACS.selection', 'age', 'cell', 'cell_ontology_class',\n",
    "#        'cell_ontology_id', 'free_annotation', 'method', 'mouse.id', 'sex',\n",
    "#        'subtissue', 'tissue', 'n_genes', 'n_counts', 'louvain', 'leiden',\n",
    "#        'batch']\n",
    "# all indexes.\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "celltype_df = pd.DataFrame(adata.obs[\"cell_ontology_class\"])\n",
    "celltype_df = celltype_df.rename(columns={\"cell_ontology_class\": \"celltype\"})\n",
    "\n",
    "\n",
    "#  Extract and display all unique cell types\n",
    "unique_cell_types = celltype_df['celltype'].unique()\n",
    "celltype_encoded = le.fit_transform(adata.obs[\"cell_ontology_class\"])\n",
    "celltype_df = pd.DataFrame(celltype_encoded, index=adata.obs.index, columns=['celltype_encoded'])\n",
    "print(celltype_df.values)\n",
    "\n",
    "\n",
    "age_df = pd.DataFrame(adata.obs[\"age\"])\n",
    "age_df = age_df.rename(columns={\"age\": \"age\"})\n",
    "\n",
    "gender_df = pd.DataFrame(adata.obs[\"sex\"])\n",
    "gender_df = gender_df.rename(columns={\"sex\": \"gender\"})\n",
    "#map male to 0, female to 1\n",
    "gender_mapping = {'male': 0, 'female': 1}\n",
    "gender_df['gender'] = gender_df['gender'].map(gender_mapping)\n",
    "\n",
    "\n",
    "\n",
    "ngenes_df = pd.DataFrame(adata.obs[\"n_genes\"])\n",
    "ngenes_df = ngenes_df.rename(columns={\"n_genes\": \"n_genes\"})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_age(age_df, substring):\n",
    "    values = []\n",
    "    for x in age_df[\"age\"]:\n",
    "        try:\n",
    "            # Attempt to strip the substring and convert to integer\n",
    "            value = int(x.strip(substring))\n",
    "            values.append(value)\n",
    "        except ValueError:\n",
    "            # Handle the case where conversion fails\n",
    "            warnings.warn(f\"Warning: '{x}' could not be converted to an integer.\")\n",
    "            break\n",
    "    age_df[\"age\"] = values\n",
    "    return age_df\n",
    "\n",
    "def get_raw_counts(adata, celltype_df):\n",
    "    raw_count = pd.DataFrame.sparse.from_spmatrix(adata.X.T, \n",
    "                                               index = adata.var_names, \n",
    "                                               columns = adata.obs_names).astype(int)\n",
    "    raw_count = raw_count[list(celltype_df.index)]\n",
    "    return raw_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_age_df = clean_age(age_df, \"m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 2, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_count = get_raw_counts(adata, celltype_df)\n",
    "raw_count.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index          A10_B001060_B009250_S214.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         2   \n",
      "0610007L01Rik                                         0   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         0   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         0   \n",
      "\n",
      "index          A10_B001061_B009251_S298.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         0   \n",
      "0610007L01Rik                                         0   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         0   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         0   \n",
      "\n",
      "index          A10_B002702_B009296_S154.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         0   \n",
      "0610007L01Rik                                         0   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         0   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         0   \n",
      "\n",
      "index          A11_B001060_B009250_S215.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         2   \n",
      "0610007L01Rik                                         0   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         1   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         0   \n",
      "\n",
      "index          A11_B001061_B009251_S299.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         0   \n",
      "0610007L01Rik                                         0   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         0   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         0   \n",
      "\n",
      "index          A11_B002518_B009295_S71.mm10-plus-1-0-0  \\\n",
      "index                                                    \n",
      "0610005C13Rik                                        0   \n",
      "0610007C21Rik                                        1   \n",
      "0610007L01Rik                                        0   \n",
      "0610007N19Rik                                        0   \n",
      "0610007P08Rik                                        0   \n",
      "0610007P14Rik                                        0   \n",
      "0610007P22Rik                                        4   \n",
      "0610008F07Rik                                        0   \n",
      "0610009B14Rik                                        0   \n",
      "0610009B22Rik                                        0   \n",
      "\n",
      "index          A11_D045853_B009304_S107.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         1   \n",
      "0610007L01Rik                                         0   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         0   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         2   \n",
      "\n",
      "index          A12_B001060_B009250_S216.mm10-plus-1-0-0  \\\n",
      "index                                                     \n",
      "0610005C13Rik                                         0   \n",
      "0610007C21Rik                                         0   \n",
      "0610007L01Rik                                         3   \n",
      "0610007N19Rik                                         0   \n",
      "0610007P08Rik                                         0   \n",
      "0610007P14Rik                                         1   \n",
      "0610007P22Rik                                         0   \n",
      "0610008F07Rik                                         0   \n",
      "0610009B14Rik                                         0   \n",
      "0610009B22Rik                                         3   \n",
      "\n",
      "index          A12_B002518_B009295_S72.mm10-plus-1-0-0  \\\n",
      "index                                                    \n",
      "0610005C13Rik                                        0   \n",
      "0610007C21Rik                                        1   \n",
      "0610007L01Rik                                        0   \n",
      "0610007N19Rik                                        0   \n",
      "0610007P08Rik                                        0   \n",
      "0610007P14Rik                                        0   \n",
      "0610007P22Rik                                        0   \n",
      "0610008F07Rik                                        0   \n",
      "0610009B14Rik                                        0   \n",
      "0610009B22Rik                                        0   \n",
      "\n",
      "index          A12_B002702_B009296_S156.mm10-plus-1-0-0  ...  \\\n",
      "index                                                    ...   \n",
      "0610005C13Rik                                         0  ...   \n",
      "0610007C21Rik                                         2  ...   \n",
      "0610007L01Rik                                         0  ...   \n",
      "0610007N19Rik                                         0  ...   \n",
      "0610007P08Rik                                         0  ...   \n",
      "0610007P14Rik                                         0  ...   \n",
      "0610007P22Rik                                         0  ...   \n",
      "0610008F07Rik                                         0  ...   \n",
      "0610009B14Rik                                         0  ...   \n",
      "0610009B22Rik                                         0  ...   \n",
      "\n",
      "index          P8.MAA000923.3_9_M.1.1-1-1-1  P8.MAA000925.3_9_M.1.1-1-1-1  \\\n",
      "index                                                                       \n",
      "0610005C13Rik                             0                             0   \n",
      "0610007C21Rik                             1                             0   \n",
      "0610007L01Rik                             1                             0   \n",
      "0610007N19Rik                             0                             0   \n",
      "0610007P08Rik                             1                             0   \n",
      "0610007P14Rik                             0                             0   \n",
      "0610007P22Rik                             0                             0   \n",
      "0610008F07Rik                             0                             0   \n",
      "0610009B14Rik                             0                             0   \n",
      "0610009B22Rik                             0                             0   \n",
      "\n",
      "index          P8.MAA000926.3_9_M.1.1-1-1-1  P8.MAA000932.3_11_M.1.1-1-1-1  \\\n",
      "index                                                                        \n",
      "0610005C13Rik                             0                              0   \n",
      "0610007C21Rik                             0                              0   \n",
      "0610007L01Rik                             0                              1   \n",
      "0610007N19Rik                             0                              0   \n",
      "0610007P08Rik                             0                              0   \n",
      "0610007P14Rik                             0                              0   \n",
      "0610007P22Rik                             0                              0   \n",
      "0610008F07Rik                             0                              0   \n",
      "0610009B14Rik                             0                              0   \n",
      "0610009B22Rik                             0                              0   \n",
      "\n",
      "index          P9.MAA000560.3_10_M.1.1-1-1-1  P9.MAA000564.3_10_M.1.1-1-1-1  \\\n",
      "index                                                                         \n",
      "0610005C13Rik                              0                              0   \n",
      "0610007C21Rik                              1                              0   \n",
      "0610007L01Rik                              0                              0   \n",
      "0610007N19Rik                              0                              0   \n",
      "0610007P08Rik                              0                              0   \n",
      "0610007P14Rik                              0                              0   \n",
      "0610007P22Rik                              0                              0   \n",
      "0610008F07Rik                              0                              0   \n",
      "0610009B14Rik                              0                              0   \n",
      "0610009B22Rik                              0                              0   \n",
      "\n",
      "index          P9.MAA000930.3_8_M.1.1-1-1-1  P9.MAA000932.3_11_M.1.1-1-1-1  \\\n",
      "index                                                                        \n",
      "0610005C13Rik                             0                              0   \n",
      "0610007C21Rik                             1                              1   \n",
      "0610007L01Rik                             0                              1   \n",
      "0610007N19Rik                             0                              0   \n",
      "0610007P08Rik                             0                              0   \n",
      "0610007P14Rik                             1                              0   \n",
      "0610007P22Rik                             0                              0   \n",
      "0610008F07Rik                             0                              0   \n",
      "0610009B14Rik                             0                              0   \n",
      "0610009B22Rik                             0                              0   \n",
      "\n",
      "index          P9.MAA000935.3_8_M.1.1-1-1-1  P9.MAA001894.3_39_F.1.1-1-1-1  \n",
      "index                                                                       \n",
      "0610005C13Rik                             0                              0  \n",
      "0610007C21Rik                             1                              0  \n",
      "0610007L01Rik                             2                              0  \n",
      "0610007N19Rik                             0                              0  \n",
      "0610007P08Rik                             0                              0  \n",
      "0610007P14Rik                             2                              3  \n",
      "0610007P22Rik                             0                              0  \n",
      "0610008F07Rik                             0                              0  \n",
      "0610009B14Rik                             0                              0  \n",
      "0610009B22Rik                             0                              0  \n",
      "\n",
      "[10 rows x 15692 columns]\n",
      "                                          age\n",
      "index                                        \n",
      "A10_B001060_B009250_S214.mm10-plus-1-0-0   18\n",
      "A10_B001061_B009251_S298.mm10-plus-1-0-0   18\n",
      "A10_B002702_B009296_S154.mm10-plus-1-0-0   18\n",
      "A11_B001060_B009250_S215.mm10-plus-1-0-0   18\n",
      "A11_B001061_B009251_S299.mm10-plus-1-0-0   18\n",
      "A11_B002518_B009295_S71.mm10-plus-1-0-0    18\n",
      "A11_D045853_B009304_S107.mm10-plus-1-0-0   18\n",
      "A12_B001060_B009250_S216.mm10-plus-1-0-0   18\n",
      "A12_B002518_B009295_S72.mm10-plus-1-0-0    18\n",
      "A12_B002702_B009296_S156.mm10-plus-1-0-0   18\n",
      "                                         gender\n",
      "index                                          \n",
      "A10_B001060_B009250_S214.mm10-plus-1-0-0      1\n",
      "A10_B001061_B009251_S298.mm10-plus-1-0-0      1\n",
      "A10_B002702_B009296_S154.mm10-plus-1-0-0      0\n",
      "A11_B001060_B009250_S215.mm10-plus-1-0-0      1\n",
      "A11_B001061_B009251_S299.mm10-plus-1-0-0      1\n",
      "A11_B002518_B009295_S71.mm10-plus-1-0-0       0\n",
      "A11_D045853_B009304_S107.mm10-plus-1-0-0      1\n",
      "A12_B001060_B009250_S216.mm10-plus-1-0-0      1\n",
      "A12_B002518_B009295_S72.mm10-plus-1-0-0       0\n",
      "A12_B002702_B009296_S156.mm10-plus-1-0-0      0\n",
      "                                          celltype_encoded\n",
      "index                                                     \n",
      "A10_B001060_B009250_S214.mm10-plus-1-0-0                 3\n",
      "A10_B001061_B009251_S298.mm10-plus-1-0-0                 3\n",
      "A10_B002702_B009296_S154.mm10-plus-1-0-0                 3\n",
      "A11_B001060_B009250_S215.mm10-plus-1-0-0                 3\n",
      "A11_B001061_B009251_S299.mm10-plus-1-0-0                 3\n",
      "A11_B002518_B009295_S71.mm10-plus-1-0-0                  3\n",
      "A11_D045853_B009304_S107.mm10-plus-1-0-0                 3\n",
      "A12_B001060_B009250_S216.mm10-plus-1-0-0                 3\n",
      "A12_B002518_B009295_S72.mm10-plus-1-0-0                  3\n",
      "A12_B002702_B009296_S156.mm10-plus-1-0-0                 3\n"
     ]
    }
   ],
   "source": [
    "print(raw_count[:10])\n",
    "print(age_df[:10])\n",
    "print(gender_df[:10])\n",
    "print(celltype_df[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_count_T: (15692, 21026)\n",
      "Shape of age_df: (15692, 1)\n",
      "Shape of gender_df: (15692, 1)\n",
      "Shape of celltype_df: (15692, 1)\n"
     ]
    }
   ],
   "source": [
    "# now concatenate all the dataframes together\n",
    "# Transpose raw_count so that cells are the index\n",
    "raw_count_T = raw_count.T\n",
    "\n",
    "\n",
    "cells_in_raw = set(raw_count_T.index)\n",
    "cells_in_age = set(age_df.index)\n",
    "cells_in_gender = set(gender_df.index)\n",
    "cells_in_celltype = set(celltype_df.index)\n",
    "\n",
    "# Find common cells present in all DataFrames\n",
    "\n",
    "print(\"Shape of raw_count_T:\", raw_count_T.shape)\n",
    "print(\"Shape of age_df:\", age_df.shape)\n",
    "print(\"Shape of gender_df:\", gender_df.shape)\n",
    "print(\"Shape of celltype_df:\", celltype_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, filter DataFrames to include only common cells\n",
    "common_cells = cells_in_raw  & cells_in_gender & cells_in_celltype & cells_in_age\n",
    "common_cells_list = list(common_cells)\n",
    "\n",
    "# Filter DataFrames to include only common cells\n",
    "raw_count_T = raw_count_T.loc[common_cells_list]\n",
    "age_df = age_df.loc[common_cells_list]\n",
    "gender_df = gender_df.loc[common_cells_list]\n",
    "celltype_df = celltype_df.loc[common_cells_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames for deep learning\n",
    "combined_df = pd.concat([raw_count_T,  gender_df, celltype_df, age_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene expression data matches: True\n",
      "Age data matches: True\n",
      "Gender data matches: True\n",
      "\n",
      "Overall data matches: True\n",
      "\n",
      "All data in combined_df matches the original DataFrames.\n"
     ]
    }
   ],
   "source": [
    "# validate the combined_df with the original dataframes\n",
    "# the cell type is marked out since it has been changed to numbers\n",
    "\n",
    "#  Ensure DataFrames are properly aligned\n",
    "def prepare_dataframe(df):\n",
    "    df.index = df.index.astype(str).str.strip().str.lower()\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "raw_count_T = prepare_dataframe(raw_count_T)\n",
    "age_df = prepare_dataframe(age_df)\n",
    "gender_df = prepare_dataframe(gender_df)\n",
    "celltype_df = prepare_dataframe(celltype_df)\n",
    "combined_df = prepare_dataframe(combined_df)\n",
    "\n",
    "# Ensure that indices match across all DataFrames\n",
    "common_indices = raw_count_T.index.intersection(age_df.index).intersection(gender_df.index).intersection(celltype_df.index)\n",
    "raw_count_T = raw_count_T.loc[common_indices]\n",
    "age_df = age_df.loc[common_indices]\n",
    "gender_df = gender_df.loc[common_indices]\n",
    "celltype_df = celltype_df.loc[common_indices]\n",
    "combined_df = combined_df.loc[common_indices]\n",
    "\n",
    "#  Compare Gene Expression Data\n",
    "gene_columns = raw_count_T.columns\n",
    "combined_gene_data = combined_df[gene_columns]\n",
    "gene_data_matches = combined_gene_data.equals(raw_count_T)\n",
    "print(\"Gene expression data matches:\", gene_data_matches)\n",
    "\n",
    "#  Compare Metadata Columns\n",
    "age_matches = combined_df['age'].equals(age_df['age'])\n",
    "print(\"Age data matches:\", age_matches)\n",
    "\n",
    "gender_matches = combined_df['gender'].equals(gender_df['gender'])\n",
    "print(\"Gender data matches:\", gender_matches)\n",
    "\n",
    "# celltype_matches = combined_df['celltype'].equals(celltype_df['celltype'])\n",
    "# print(\"Cell type data matches:\", celltype_matches)\n",
    "\n",
    "#  Report Overall Match\n",
    "all_data_matches = gene_data_matches and age_matches and gender_matches #and celltype_matches\n",
    "print(\"\\nOverall data matches:\", all_data_matches)\n",
    "\n",
    "# Step 5: Identify and Report Discrepancies\n",
    "if not all_data_matches:\n",
    "    if not gene_data_matches:\n",
    "        # Identify discrepancies in gene expression data\n",
    "        gene_diff = (combined_gene_data != raw_count_T)\n",
    "        cells_with_diff = gene_diff.any(axis=1)\n",
    "        genes_with_diff = gene_diff.any(axis=0)\n",
    "        print(\"\\nDiscrepancies found in gene expression data.\")\n",
    "        print(f\"Number of cells with discrepancies: {cells_with_diff.sum()}\")\n",
    "        print(f\"Number of genes with discrepancies: {genes_with_diff.sum()}\")\n",
    "        # List first few discrepancies\n",
    "        discrepant_cells = cells_with_diff[cells_with_diff].index[:5]\n",
    "        for cell in discrepant_cells:\n",
    "            diff_genes = gene_diff.loc[cell][gene_diff.loc[cell]].index.tolist()\n",
    "            print(f\"Cell '{cell}' has discrepancies in genes: {diff_genes[:5]}\")\n",
    "    \n",
    "    if not age_matches:\n",
    "        age_diff = combined_df['age'] != age_df['age']\n",
    "        discrepant_cells = age_diff[age_diff].index.tolist()\n",
    "        print(\"\\nDiscrepancies found in age data for cells:\", discrepant_cells)\n",
    "    \n",
    "    if not gender_matches:\n",
    "        gender_diff = combined_df['gender'] != gender_df['gender']\n",
    "        discrepant_cells = gender_diff[gender_diff].index.tolist()\n",
    "        print(\"\\nDiscrepancies found in gender data for cells:\", discrepant_cells)\n",
    "    \n",
    "    # if not celltype_matches:\n",
    "    #     celltype_diff = combined_df['celltype'] != celltype_df['celltype']\n",
    "    #     discrepant_cells = celltype_diff[celltype_diff].index.tolist()\n",
    "    #     print(\"\\nDiscrepancies found in cell type data for cells:\", discrepant_cells)\n",
    "else:\n",
    "    print(\"\\nAll data in combined_df matches the original DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 1 3 3]\n",
      " [0 0 0 ... 1 3 3]\n",
      " [0 0 0 ... 1 3 3]\n",
      " ...\n",
      " [0 0 3 ... 0 3 3]\n",
      " [0 0 0 ... 0 3 3]\n",
      " [0 2 0 ... 0 3 3]]\n"
     ]
    }
   ],
   "source": [
    "print(combined_df[:10].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (12553, 21028)\n",
      "Test features shape: (3139, 21028)\n",
      "Training target shape: (12553,)\n",
      "Test target shape: (3139,)\n"
     ]
    }
   ],
   "source": [
    "# pre process data for machine learning\n",
    "# sperate data into training and testing sets\n",
    "\n",
    "data = combined_df.values\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "x = data[:, :-1]  # All columns except the last one\n",
    "y = data[:, -1]   # The last column\n",
    "\n",
    "# Use LabelEncoder to encode labels to 0, 1, 2\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "# optional, balance the class weights since the data is imbalanced\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Update criterion\n",
    "class_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Split data into training and test sets (80% training, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(\"Training features shape:\", x_train.shape)\n",
    "print(\"Test features shape:\", x_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Test target shape:\", y_test.shape)\n",
    "\n",
    "# Convert tensors to NumPy for preprocessing\n",
    "X_train_np = x_train\n",
    "X_test_np = x_test\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_np[:, :-2] = scaler.fit_transform(X_train_np[:, :-2])\n",
    "X_test_np[:, :-2] = scaler.transform(X_test_np[:, :-2])\n",
    "\n",
    "# Convert back to tensors\n",
    "X_train = torch.from_numpy(X_train_np).float()\n",
    "X_test = torch.from_numpy(X_test_np).float()\n",
    "\n",
    "# Reduce gene expression features to 100 components\n",
    "pca = PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train_np[:, :-2])\n",
    "X_test_pca = pca.transform(X_test_np[:, :-2])\n",
    "\n",
    "# Concatenate the last two features back\n",
    "X_train_pca = np.concatenate((X_train_pca, X_train_np[:, -2:]), axis=1)\n",
    "X_test_pca = np.concatenate((X_test_pca, X_test_np[:, -2:]), axis=1)\n",
    "\n",
    "# Convert back to tensors\n",
    "X_train_pca = torch.from_numpy(X_train_pca).float()\n",
    "X_test_pca = torch.from_numpy(X_test_pca).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12553, 21028)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to tensor dataset\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "train_dataset = TensorDataset(X_train_pca, y_train)\n",
    "test_dataset = TensorDataset(X_test_pca, y_test)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define model, loss function, optimizer\n",
    "input_size = X_train_pca.shape[1]\n",
    "y = torch.from_numpy(y).long()\n",
    "num_classes = len(torch.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# networks\n",
    "# use a MLP as a start point\n",
    "\n",
    "################################################################################\n",
    "## simple MLP\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 128)\n",
    "#         self.dropout1 = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.dropout2 = nn.Dropout(0.5)\n",
    "#         self.fc3 = nn.Linear(64, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Weight the last two features more heavily\n",
    "#         x[:, -2:] *= 5.0  # Adjust the weighting factor as needed\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "################################################################################    \n",
    "\n",
    "\n",
    "################################################################################    \n",
    "# slightly more complex MLP with out weights\n",
    "# result log: acc: 92% r2: 0.83\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 128)\n",
    "#         self.dropout1 = nn.Dropout(0.3)\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.dropout2 = nn.Dropout(0.3)\n",
    "#         self.fc3 = nn.Linear(64, 32)\n",
    "#         self.dropout3 = nn.Dropout(0.3)\n",
    "#         self.fc4 = nn.Linear(32, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x[:, -2:] *= 5.0  # Adjust the weighting factor as needed\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.fc4(x)\n",
    "#         return x\n",
    "################################################################################    \n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# a configurable MLP\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, depth, base_exp=7, decay_factor=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.base_exp = base_exp\n",
    "        \n",
    "        # Generate hidden layer sizes as powers of 2\n",
    "        # Starts from 2^base_exp and decreases by 1 exponent with each layer\n",
    "        hidden_sizes = [2 ** (self.base_exp - int(i/decay_factor)) for i in range(depth - 1)]\n",
    "        hidden_sizes.append(32)  # Last hidden layer has 32 neurons\n",
    "        print(\"Hidden layer sizes:\", hidden_sizes)\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        in_features = input_size\n",
    "        \n",
    "        # Create hidden layers and dropout layers\n",
    "        for out_features in hidden_sizes:\n",
    "            self.fcs.append(nn.Linear(in_features, out_features))\n",
    "            self.dropouts.append(nn.Dropout(0.3))\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x[:, -2:] *= 5.0  # Adjust the weighting factor as needed\n",
    "        \n",
    "        for i in range(len(self.fcs)):\n",
    "            if i < len(self.fcs) - 1:  # Apply ReLU and Dropout after all but last hidden layer\n",
    "                x = F.relu(self.fcs[i](x))\n",
    "                x = self.dropouts[i](x)\n",
    "            else:  # For the last hidden layer, do not apply ReLU\n",
    "                x = self.fcs[i](x)\n",
    "                x = self.dropouts[i](x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "## transformer\n",
    "# I fixed the lenght of the gene sequence to 100, should change later \n",
    "\n",
    "# result log: acc: 88%, r2:0.76\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         # Gene sequence parameters\n",
    "#         gene_seq_length = 100  # Since we have 100 gene features\n",
    "#         embedding_dim = 16     # Adjust based on your preference\n",
    "\n",
    "#         # Embedding layer for gene sequence features\n",
    "#         self.embedding = nn.Linear(1, embedding_dim)\n",
    "\n",
    "#         # Positional Encoding\n",
    "#         self.positional_encoding = nn.Parameter(torch.zeros(1, gene_seq_length, embedding_dim))\n",
    "\n",
    "#         # Transformer Encoder\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "#         # Processing the last two features\n",
    "#         self.fc_aux = nn.Sequential(\n",
    "#             nn.Linear(2, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(16, 16),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#         # Final fully connected layer\n",
    "#         self.fc_out = nn.Linear(gene_seq_length * embedding_dim + 16, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Split the input into gene sequence features and auxiliary features\n",
    "#         gene_seq_features = x[:, :100]  # (batch_size, 100)\n",
    "#         aux_features = x[:, 100:]       # (batch_size, 2)\n",
    "\n",
    "#         batch_size = gene_seq_features.size(0)\n",
    "\n",
    "#         # Reshape gene sequence features to (batch_size, seq_length, 1)\n",
    "#         gene_seq_features = gene_seq_features.unsqueeze(-1)  # (batch_size, 100, 1)\n",
    "\n",
    "#         # Embedding\n",
    "#         gene_seq_features = self.embedding(gene_seq_features)  # (batch_size, 100, embedding_dim)\n",
    "\n",
    "#         # Add positional encoding\n",
    "#         gene_seq_features = gene_seq_features + self.positional_encoding  # (batch_size, 100, embedding_dim)\n",
    "\n",
    "#         # Transformer expects input shape (seq_length, batch_size, embedding_dim)\n",
    "#         gene_seq_features = gene_seq_features.permute(1, 0, 2)  # (100, batch_size, embedding_dim)\n",
    "\n",
    "#         # Pass through Transformer Encoder\n",
    "#         gene_seq_features = self.transformer_encoder(gene_seq_features)\n",
    "\n",
    "#         # Permute back to (batch_size, seq_length, embedding_dim)\n",
    "#         gene_seq_features = gene_seq_features.permute(1, 0, 2)\n",
    "\n",
    "#         # Flatten gene sequence features\n",
    "#         gene_seq_features = gene_seq_features.contiguous().view(batch_size, -1)\n",
    "\n",
    "#         # Process the auxiliary features\n",
    "#         aux_features = self.fc_aux(aux_features)  # (batch_size, 16)\n",
    "\n",
    "#         # Concatenate gene sequence features and auxiliary features\n",
    "#         x = torch.cat([gene_seq_features, aux_features], dim=1)  # (batch_size, total_features)\n",
    "\n",
    "#         # Output layer\n",
    "#         x = self.fc_out(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder and decoder structure\n",
    "# instead of doing pca, we can use autoencoder to reduce the dimensionality of the data\n",
    "# seems not work, the dimension seems too high for the autoencoder to work \n",
    "\n",
    "# class Autoencoder(nn.Module):\n",
    "#     def __init__(self, input_dim, latent_dim=128, aux_dim=2):\n",
    "#         super(Autoencoder, self).__init__()\n",
    "#         gene_dim = input_dim - aux_dim  # Dimension of gene expression data\n",
    "        \n",
    "#         # Encoder for gene expression data\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(gene_dim, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(4096, 2048),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(2048, 1024),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(512, latent_dim)\n",
    "#         )\n",
    "        \n",
    "#         # Decoder for gene expression data\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(latent_dim + aux_dim, 512),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 2048),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(4096, gene_dim),\n",
    "#             nn.Sigmoid()  # Use Sigmoid if gene data is normalized between 0 and 1\n",
    "#         )\n",
    "        \n",
    "#         # Auxiliary feature processor\n",
    "#         self.aux_processor = nn.Sequential(\n",
    "#             nn.Linear(aux_dim, aux_dim * 8),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(aux_dim * 8, aux_dim * 8),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(aux_dim * 8, aux_dim * 4),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(aux_dim * 4, aux_dim * 2),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(aux_dim * 2, aux_dim),\n",
    "#             nn.ReLU(True)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Split the input into gene expression data and auxiliary features\n",
    "#         gene_data = x[:, :-2]  # (batch_size, gene_dim)\n",
    "#         aux_features = x[:, -2:]  # (batch_size, aux_dim)\n",
    "        \n",
    "#         # Process auxiliary features\n",
    "#         aux_processed = self.aux_processor(aux_features)\n",
    "        \n",
    "#         # Encode gene expression data\n",
    "#         latent = self.encoder(gene_data)\n",
    "        \n",
    "#         # Concatenate latent representation with auxiliary features\n",
    "#         combined = torch.cat((latent, aux_processed), dim=1)\n",
    "        \n",
    "#         # Decode to reconstruct gene expression data\n",
    "#         reconstructed_gene = self.decoder(combined)\n",
    "        \n",
    "#         # Optionally, reconstruct auxiliary features (if desired)\n",
    "#         # For now, we assume we only reconstruct gene expression data\n",
    "        \n",
    "#         # Combine reconstructed gene data with original auxiliary features\n",
    "#         output = torch.cat((reconstructed_gene, aux_features), dim=1)\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "# # convert data to tensor dataset differently if pca is not used\n",
    "# y_train = torch.from_numpy(y_train).long()\n",
    "# y_test = torch.from_numpy(y_test).long()\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# # Data loaders\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# # Define model, loss function, optimizer\n",
    "# input_size = X_train.shape[1]\n",
    "# y = torch.from_numpy(y).long()\n",
    "# num_classes = len(torch.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 3\n",
      "Hidden layer sizes: [128.0, 90.50966799187809, 64.0, 45.254833995939045, 32]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_size, num_classes)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model = Net(input_size, num_classes)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#model = Autoencoder(input_size, latent_dim=128, aux_dim=2)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_exp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[95], line 78\u001b[0m, in \u001b[0;36mNet.__init__\u001b[1;34m(self, input_size, num_classes, depth, base_exp, decay_factor)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Create hidden layers and dropout layers\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out_features \u001b[38;5;129;01min\u001b[39;00m hidden_sizes:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropouts\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m))\n\u001b[0;32m     80\u001b[0m     in_features \u001b[38;5;241m=\u001b[39m out_features\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "print(input_size, num_classes)\n",
    "#model = Net(input_size, num_classes)\n",
    "#model = Autoencoder(input_size, latent_dim=128, aux_dim=2)\n",
    "model = Net(input_size, num_classes, depth=5, base_exp=7, decay_factor=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = class_criterion(outputs, labels) # if you want to balance the class weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 92.55%\n"
     ]
    }
   ],
   "source": [
    "# evlaute the model\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        #print(labels)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - R²: 0.9237\n",
      "Class 1 - R²: 0.7814\n",
      "Class 2 - R²: 0.7124\n",
      "Overall R²: 0.8320\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # For classification probabilities\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        # Assuming labels are on CPU\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        # Collect probabilities or predicted class indices\n",
    "        all_preds.extend(probabilities.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "all_preds = np.array(all_preds)        # Shape: [num_samples, num_classes]\n",
    "all_labels = np.array(all_labels)      # Shape: [num_samples]\n",
    "\n",
    "# Number of classes\n",
    "num_classes = all_preds.shape[1]\n",
    "\n",
    "# Convert true labels to one-hot encoding\n",
    "all_labels_one_hot = label_binarize(all_labels, classes=range(num_classes))\n",
    "\n",
    "\n",
    "\n",
    "r_squared_values = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    y_true = all_labels_one_hot[:, i]\n",
    "    y_pred = all_preds[:, i]\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    r_squared_values.append(r_squared)\n",
    "    print(f\"Class {i} - R²: {r_squared:.4f}\")\n",
    "\n",
    "\n",
    "# Flatten the arrays\n",
    "y_true_flat = all_labels_one_hot.flatten()\n",
    "y_pred_flat = all_preds.flatten()\n",
    "\n",
    "# Calculate R²\n",
    "overall_r_squared = r2_score(y_true_flat, y_pred_flat)\n",
    "print(f\"Overall R²: {overall_r_squared:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
